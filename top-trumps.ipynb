{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23ccc23",
   "metadata": {},
   "source": [
    "# Automatic Game Balancing with Top Trumps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35c4a4",
   "metadata": {},
   "source": [
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef047487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: pymoo in /opt/homebrew/lib/python3.10/site-packages (0.6.1.6)\n",
      "Requirement already satisfied: scipy>=1.1 in /opt/homebrew/lib/python3.10/site-packages (from pymoo) (1.15.2)\n",
      "Requirement already satisfied: moocore>=0.1.7 in /opt/homebrew/lib/python3.10/site-packages (from pymoo) (0.1.10)\n",
      "Requirement already satisfied: autograd>=1.4 in /opt/homebrew/lib/python3.10/site-packages (from pymoo) (1.8.0)\n",
      "Requirement already satisfied: cma>=3.2.2 in /opt/homebrew/lib/python3.10/site-packages (from pymoo) (4.4.1)\n",
      "Requirement already satisfied: matplotlib>=3 in /opt/homebrew/lib/python3.10/site-packages (from pymoo) (3.10.1)\n",
      "Requirement already satisfied: alive_progress in /opt/homebrew/lib/python3.10/site-packages (from pymoo) (3.3.0)\n",
      "Requirement already satisfied: Deprecated in /opt/homebrew/lib/python3.10/site-packages (from pymoo) (1.2.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3->pymoo) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3->pymoo) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3->pymoo) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3->pymoo) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3->pymoo) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3->pymoo) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3->pymoo) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib>=3->pymoo) (2.9.0.post0)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /opt/homebrew/lib/python3.10/site-packages (from moocore>=0.1.7->pymoo) (1.17.1)\n",
      "Requirement already satisfied: platformdirs in /opt/homebrew/lib/python3.10/site-packages (from moocore>=0.1.7->pymoo) (4.5.1)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.10/site-packages (from cffi>=1.17.1->moocore>=0.1.7->pymoo) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo) (1.17.0)\n",
      "Requirement already satisfied: about-time==4.2.1 in /opt/homebrew/lib/python3.10/site-packages (from alive_progress->pymoo) (4.2.1)\n",
      "Requirement already satisfied: graphemeu==0.7.2 in /opt/homebrew/lib/python3.10/site-packages (from alive_progress->pymoo) (0.7.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/homebrew/lib/python3.10/site-packages (from Deprecated->pymoo) (1.14.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pymoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8gc4aj4vjh",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "All configurable parameters for the optimization run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "h0u4ffafpv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Problem: 22 cards × 4 categories = 88 variables\n",
      "  Simulations per evaluation: 1000 (FINAL)\n",
      "  Algorithm: NSGA-II with pop_size=100, n_gen=100\n",
      "  Total evaluations: ~10,000\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# Problem parameters (from assignment)\n",
    "NUM_CARDS = 22              # K: number of cards in deck\n",
    "NUM_CATEGORIES = 4          # L: number of categories per card\n",
    "N_SIMULATIONS = 1000        # R: simulations per evaluation (final run)\n",
    "N_SIMULATIONS_DEV = 100     # R: simulations per evaluation (development)\n",
    "VALUE_BOUNDS = (1.0, 10.0)  # Card value bounds\n",
    "\n",
    "# Algorithm parameters\n",
    "POP_SIZE = 100              # Population size for NSGA-II\n",
    "N_GEN = 100                 # Number of generations\n",
    "SEED = 42                   # Random seed for reproducibility\n",
    "CROSSOVER_PROB = 0.9        # SBX crossover probability\n",
    "CROSSOVER_ETA = 15          # SBX distribution index\n",
    "MUTATION_ETA = 20           # Polynomial mutation distribution index\n",
    "\n",
    "# Runtime settings\n",
    "DEV_MODE = False             # True: use N_SIMULATIONS_DEV (faster), False: use N_SIMULATIONS\n",
    "VERBOSE = True              # Print progress during optimization\n",
    "SAVE_HISTORY = True         # Store algorithm state each generation (for convergence analysis)\n",
    "\n",
    "# Output settings\n",
    "OUTPUT_DIR = \"./results\"    # Directory for saving results\n",
    "\n",
    "# Derived values\n",
    "N_VAR = NUM_CARDS * NUM_CATEGORIES  # Total decision variables (88)\n",
    "R = N_SIMULATIONS_DEV if DEV_MODE else N_SIMULATIONS\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Problem: {NUM_CARDS} cards × {NUM_CATEGORIES} categories = {N_VAR} variables\")\n",
    "print(f\"  Simulations per evaluation: {R} ({'DEV' if DEV_MODE else 'FINAL'})\")\n",
    "print(f\"  Algorithm: NSGA-II with pop_size={POP_SIZE}, n_gen={N_GEN}\")\n",
    "print(f\"  Total evaluations: ~{POP_SIZE * N_GEN:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04138451",
   "metadata": {},
   "source": [
    "## Top Trumps Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd6f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class TopTrumpsSimulation:\n",
    "    \"\"\"TopTrumpsSimulation implements the basic TopTrumps game engine and agent-based simulation.\"\"\"\n",
    "\n",
    "    def __init__(self, num_cards=52, num_categories=6):\n",
    "        self.K = num_cards  # Number of cards\n",
    "        self.L = num_categories  # Number of categories\n",
    "\n",
    "        self.set_random_deck()\n",
    "\n",
    "    def _normalize_deck(self, deck):\n",
    "        \"\"\"Normalizes the deck such that each property is in the range [0,1]. This is required for the p0 agent.\"\"\"\n",
    "        min_values = [0] * self.L\n",
    "        max_values = [0] * self.L\n",
    "\n",
    "        for cat in range(self.L):\n",
    "            all_values = [card[cat] for card in deck]\n",
    "            min_values[cat] = min(all_values)\n",
    "            max_values[cat] = max(all_values)\n",
    "\n",
    "        normalized_deck = []\n",
    "\n",
    "        for card in deck:\n",
    "            normalized_card = [0] * self.L\n",
    "            for cat in range(self.L):\n",
    "                normalized_card[cat] = (card[cat] - min_values[cat]) / (max_values[cat] - min_values[cat])\n",
    "            normalized_deck.append(normalized_card)\n",
    "\n",
    "        return normalized_deck\n",
    "\n",
    "    def set_deck(self, deck_list):\n",
    "        \"\"\"Creates a deck from deck_list of length K*L.\"\"\"\n",
    "\n",
    "        assert len(deck_list) == self.K * self.L\n",
    "\n",
    "        deck = np.array_split(deck_list, self.K)\n",
    "\n",
    "        self.deck = deck\n",
    "        self._normalized_deck = self._normalize_deck(deck)\n",
    "\n",
    "    def set_random_deck(self, value_range = (1, 10)):\n",
    "        \"\"\"Generates a random deck with values in value_range.\"\"\"\n",
    "        deck_list = np.random.uniform(value_range[0], value_range[1], self.K * self.L)\n",
    "        self.set_deck(deck_list)\n",
    "\n",
    "    def get_p0_choice(self, card):\n",
    "        \"\"\"p0 assumes uniform distribution between max and min values per property and then picks the best value on current card.\"\"\"\n",
    "        return card.index(max(card))\n",
    "\n",
    "    def get_p4_choice(self, card, remaining_cards):\n",
    "        \"\"\"p4 calculates exact win probability based on remaining cards.\"\"\"\n",
    "        best_prob = -1\n",
    "        best_cat = 0\n",
    "\n",
    "        for cat_idx in range(self.L):\n",
    "            my_val = card[cat_idx]\n",
    "            # Count how many cards in the potential card pool have a lower value\n",
    "            wins = sum(1 for opp_card in remaining_cards if my_val > opp_card[cat_idx])\n",
    "            prob = wins / len(remaining_cards)\n",
    "\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_cat = cat_idx\n",
    "        # print(f\"Best prob: {best_prob:.2f}, cat: {best_cat}\")\n",
    "        return best_cat\n",
    "\n",
    "    def simulate_game(self):\n",
    "        # Shuffle and distribute cards\n",
    "        temp_deck = list(self._normalized_deck)\n",
    "        random.shuffle(temp_deck)\n",
    "\n",
    "        # p4 vs p0 setup\n",
    "        p4_hand = temp_deck[:self.K//2]\n",
    "        p0_hand = temp_deck[self.K//2:]\n",
    "\n",
    "        # Track unplayed cards remaining in the 'unplayed' pool for p4's knowledge \n",
    "        all_unplayed = list(temp_deck) \n",
    "\n",
    "        p4_tricks = 0\n",
    "        current_turn = random.choice(['p4', 'p0']) # Random starting player\n",
    "        trick_changes = 0\n",
    "\n",
    "        # Game ends after all cards played once\n",
    "        for i in range(self.K // 2):\n",
    "            card_p4 = p4_hand[i]\n",
    "            card_p0 = p0_hand[i]\n",
    "\n",
    "            # Category Selection\n",
    "            if current_turn == 'p4':\n",
    "                category = self.get_p4_choice(card_p4, all_unplayed)\n",
    "            else:\n",
    "                category = self.get_p0_choice(card_p0)\n",
    "\n",
    "            # Determine Round Winner\n",
    "            val_p4 = card_p4[category]\n",
    "            val_p0 = card_p0[category]\n",
    "\n",
    "            # Remove current p0, p4 card from p4's tracking memory\n",
    "            all_unplayed.remove(card_p0)\n",
    "            all_unplayed.remove(card_p4)\n",
    "\n",
    "            if val_p4 > val_p0:\n",
    "                # print(\"P4 wins!\")\n",
    "                p4_tricks += 1\n",
    "                if current_turn == 'p0':\n",
    "                    current_turn = 'p4'\n",
    "                    trick_changes += 1\n",
    "            elif val_p0 > val_p4:\n",
    "                # print(\"P0 wins!\")\n",
    "                if current_turn == 'p4':\n",
    "                    current_turn = 'p0'\n",
    "                    trick_changes += 1\n",
    "            # In case of draw, current player keeps the lead\n",
    "\n",
    "        return {\n",
    "            \"p4_tricks\": p4_tricks,\n",
    "            \"trick_changes\": trick_changes,\n",
    "            \"p4_won\": p4_tricks > (self.K / 4) # p4 wins if they got more than half the tricks\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4422a",
   "metadata": {},
   "source": [
    "Test implementation on a random deck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13089907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 runs:\n",
      "p4 Win Rate: 57.20%\n",
      "Average p4 Tricks: 5.98\n",
      "Average Trick Changes: 2.29\n"
     ]
    }
   ],
   "source": [
    "num_cards = 22\n",
    "num_categories = 4\n",
    "num_repetitions = 1000\n",
    "\n",
    "sim = TopTrumpsSimulation(num_cards=num_cards, num_categories=num_categories)\n",
    "sim.set_deck(np.random.uniform(0, 10, num_cards * num_categories))\n",
    "results = [sim.simulate_game() for _ in range(num_repetitions)]\n",
    "\n",
    "avg_p4_win_rate = sum(1 for r in results if r['p4_won']) / num_repetitions\n",
    "avg_tricks = sum(r['p4_tricks'] for r in results) / num_repetitions\n",
    "avg_trick_changes = sum(r['trick_changes'] for r in results) / num_repetitions\n",
    "\n",
    "print(f\"Results over {num_repetitions} runs:\")\n",
    "print(f\"p4 Win Rate: {avg_p4_win_rate:.2%}\")\n",
    "print(f\"Average p4 Tricks: {avg_tricks:.2f}\")\n",
    "print(f\"Average Trick Changes: {avg_trick_changes:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c1c60",
   "metadata": {},
   "source": [
    "## Pymoo connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df33237",
   "metadata": {},
   "source": [
    "TopTrumpsBalancing provides a pymoo-compatible interface for TopTrumpsSimulation. Note that the objectives are already negated to result in a minimization problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bd300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "\n",
    "class TopTrumpsBalancing(ElementwiseProblem):\n",
    "    \"\"\"TopTrumpsBalancing connects TopTrumpsSimulation with the pymoo interface ElementwiseProblem.\"\"\"\n",
    "\n",
    "    def __init__(self, sim_instance, n_simulations=100):\n",
    "        self.sim = sim_instance\n",
    "        self.n_simulations = n_simulations\n",
    "        \n",
    "        # KL variables: Number of cards (K) * Categories (L) \n",
    "        n_var = self.sim.K * self.sim.L\n",
    "        \n",
    "        super().__init__(n_var=n_var,\n",
    "                         n_obj=2, # Objectives: Fairness (p4 win rate) and Excitement (# trick changes)\n",
    "                         n_constr=0,\n",
    "                         xl=1.0, # Lower bound for values\n",
    "                         xu=10.0) # Upper bound for values\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # Update simulation with the current candidate deck\n",
    "        self.sim.set_deck(x)\n",
    "        \n",
    "        win_rates = []\n",
    "        trick_changes_list = []\n",
    "        \n",
    "        # Perform multiple simulations to get an average\n",
    "        for _ in range(self.n_simulations):\n",
    "            res = self.sim.simulate_game()\n",
    "            win_rates.append(1 if res['p4_won'] else 0)\n",
    "            trick_changes_list.append(res['trick_changes'])\n",
    "        \n",
    "        # Objectives (Minimization):\n",
    "        # f1: -Average Win Rate of p4 (Fairness)\n",
    "        # f2: -Average Trick Changes (Excitement)\n",
    "        out[\"F\"] = [\n",
    "            -np.mean(win_rates), \n",
    "            -np.mean(trick_changes_list)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989f574",
   "metadata": {},
   "source": [
    "Set up simulation and problem instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1876e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem initialized:\n",
      "  Variables: 88\n",
      "  Objectives: 2\n",
      "  Bounds: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# Set up simulation and problem instance using configuration\n",
    "my_sim = TopTrumpsSimulation(num_cards=NUM_CARDS, num_categories=NUM_CATEGORIES)\n",
    "problem = TopTrumpsBalancing(my_sim, n_simulations=R)\n",
    "\n",
    "print(f\"Problem initialized:\")\n",
    "print(f\"  Variables: {problem.n_var}\")\n",
    "print(f\"  Objectives: {problem.n_obj}\")\n",
    "print(f\"  Bounds: [{problem.xl}, {problem.xu}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17267c",
   "metadata": {},
   "source": [
    "Now it's your turn: Optimize `problem` using an approach of your choice and analyze the results. Please refer to the documentation of [pymoo](https://pymoo.org) for more details. A good starting point is: <https://pymoo.org/getting_started/part_2.html#Initialize-an-Algorithm>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5793def6",
   "metadata": {},
   "source": [
    "## Optimization with NSGA-II\n",
    "\n",
    "We use NSGA-II (Non-dominated Sorting Genetic Algorithm II) to approximate the Pareto front. NSGA-II uses:\n",
    "- **Non-dominated sorting**: Solutions are ranked by dominance layers\n",
    "- **Crowding distance**: Maintains diversity within each layer\n",
    "- **SBX crossover**: Simulated Binary Crossover for real-valued variables\n",
    "- **Polynomial mutation**: For exploring the search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4oegt5rd4j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm configured:\n",
      "  NSGA-II with pop_size=100\n",
      "  SBX crossover (prob=0.9, eta=15)\n",
      "  PM mutation (eta=20)\n",
      "  Reference point for HV: [0.1 0.1]\n",
      "  Checkpoints every 10 generations\n"
     ]
    }
   ],
   "source": [
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PM\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.callback import Callback\n",
    "from pymoo.indicators.hv import Hypervolume\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Custom callback to track hypervolume, save checkpoints, and best objectives per generation\n",
    "class OptimizationCallback(Callback):\n",
    "    def __init__(self, ref_point, checkpoint_freq=10, output_dir=\"./results\"):\n",
    "        super().__init__()\n",
    "        self.ref_point = ref_point\n",
    "        self.hv_indicator = Hypervolume(ref_point=ref_point)\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "        self.output_dir = output_dir\n",
    "        self.data[\"hv\"] = []\n",
    "        self.data[\"n_eval\"] = []\n",
    "        self.data[\"best_f1\"] = []  # Best fairness (most negative = highest win rate)\n",
    "        self.data[\"best_f2\"] = []  # Best excitement (most negative = most trick changes)\n",
    "    \n",
    "    def notify(self, algorithm):\n",
    "        # Get current Pareto front approximation\n",
    "        F = algorithm.opt.get(\"F\")\n",
    "        X = algorithm.opt.get(\"X\")\n",
    "        gen = algorithm.n_gen\n",
    "        \n",
    "        # Calculate hypervolume\n",
    "        hv = self.hv_indicator.do(F)\n",
    "        self.data[\"hv\"].append(hv)\n",
    "        self.data[\"n_eval\"].append(algorithm.evaluator.n_eval)\n",
    "        self.data[\"best_f1\"].append(F[:, 0].min())\n",
    "        self.data[\"best_f2\"].append(F[:, 1].min())\n",
    "        \n",
    "        # Save checkpoint every checkpoint_freq generations\n",
    "        if gen % self.checkpoint_freq == 0:\n",
    "            np.save(f\"{self.output_dir}/checkpoint_F_gen{gen}.npy\", F)\n",
    "            np.save(f\"{self.output_dir}/checkpoint_X_gen{gen}.npy\", X)\n",
    "            print(f\"  [Checkpoint saved at generation {gen}, HV={hv:.4f}]\")\n",
    "\n",
    "# Reference point for hypervolume (must dominate all possible solutions)\n",
    "# Since objectives are negated: f1 ∈ [-1, 0], f2 ∈ [-10, 0] approximately\n",
    "REF_POINT = np.array([0.1, 0.1])  # Slightly above 0 to ensure valid HV\n",
    "\n",
    "# Checkpoint frequency (save every N generations)\n",
    "CHECKPOINT_FREQ = 10  # Save every 10 generations for safety\n",
    "\n",
    "# Initialize algorithm\n",
    "algorithm = NSGA2(\n",
    "    pop_size=POP_SIZE,\n",
    "    sampling=FloatRandomSampling(),\n",
    "    crossover=SBX(prob=CROSSOVER_PROB, eta=CROSSOVER_ETA),\n",
    "    mutation=PM(eta=MUTATION_ETA),\n",
    "    eliminate_duplicates=True\n",
    ")\n",
    "\n",
    "print(\"Algorithm configured:\")\n",
    "print(f\"  NSGA-II with pop_size={POP_SIZE}\")\n",
    "print(f\"  SBX crossover (prob={CROSSOVER_PROB}, eta={CROSSOVER_ETA})\")\n",
    "print(f\"  PM mutation (eta={MUTATION_ETA})\")\n",
    "print(f\"  Reference point for HV: {REF_POINT}\")\n",
    "print(f\"  Checkpoints every {CHECKPOINT_FREQ} generations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30139575-5dc3-44cf-af8a-54333862ad69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3ku360hyng",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "print(f\"Starting optimization...\")\n",
    "print(f\"  Expected evaluations: {POP_SIZE * N_GEN:,}\")\n",
    "print(f\"  Estimated time: {POP_SIZE * N_GEN * R * 0.001 / 60:.1f} minutes\")\n",
    "print()\n",
    "\n",
    "callback = OptimizationCallback(ref_point=REF_POINT)\n",
    "start_time = time.time()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "res = minimize(\n",
    "    problem,\n",
    "    algorithm,\n",
    "    ('n_gen', N_GEN),\n",
    "    seed=SEED,\n",
    "    callback=callback,\n",
    "    save_history=SAVE_HISTORY,\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nOptimization completed in {elapsed_time/60:.1f} minutes\")\n",
    "print(f\"  Total evaluations: {res.algorithm.evaluator.n_eval:,}\")\n",
    "print(f\"  Pareto front size: {len(res.F)} solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08p328fk4jfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "X = res.X  # Decision variables (deck configurations)\n",
    "F = res.F  # Objective values (negated: fairness, excitement)\n",
    "\n",
    "# Convert back to interpretable values (un-negate)\n",
    "fairness = -F[:, 0]      # p4 win rate (higher = more fair)\n",
    "excitement = -F[:, 1]    # avg trick changes (higher = more exciting)\n",
    "\n",
    "# Get callback data\n",
    "hv_history = res.algorithm.callback.data[\"hv\"]\n",
    "n_eval_history = res.algorithm.callback.data[\"n_eval\"]\n",
    "\n",
    "# Final hypervolume\n",
    "final_hv = hv_history[-1] if hv_history else 0\n",
    "\n",
    "print(\"Results Summary:\")\n",
    "print(f\"  Pareto front size: {len(F)}\")\n",
    "print(f\"  Final Hypervolume: {final_hv:.6f}\")\n",
    "print(f\"\\nObjective Ranges:\")\n",
    "print(f\"  Fairness (p4 win rate): [{fairness.min():.3f}, {fairness.max():.3f}]\")\n",
    "print(f\"  Excitement (trick changes): [{excitement.min():.2f}, {excitement.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59geqaa2ffi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to files\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "mode_str = \"dev\" if DEV_MODE else \"final\"\n",
    "\n",
    "# Save Pareto front (objectives)\n",
    "np.save(f\"{OUTPUT_DIR}/pareto_F_{mode_str}_{timestamp}.npy\", F)\n",
    "\n",
    "# Save decision variables (deck configurations)\n",
    "np.save(f\"{OUTPUT_DIR}/pareto_X_{mode_str}_{timestamp}.npy\", X)\n",
    "\n",
    "# Save convergence history\n",
    "np.savez(f\"{OUTPUT_DIR}/history_{mode_str}_{timestamp}.npz\",\n",
    "         hv=hv_history,\n",
    "         n_eval=n_eval_history,\n",
    "         best_f1=res.algorithm.callback.data[\"best_f1\"],\n",
    "         best_f2=res.algorithm.callback.data[\"best_f2\"])\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}/\")\n",
    "print(f\"  pareto_F_{mode_str}_{timestamp}.npy - Objective values\")\n",
    "print(f\"  pareto_X_{mode_str}_{timestamp}.npy - Deck configurations\") \n",
    "print(f\"  history_{mode_str}_{timestamp}.npz - Convergence data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lvzs53xjv0q",
   "metadata": {},
   "source": [
    "## Visualization & Analysis\n",
    "\n",
    "### Pareto Front\n",
    "\n",
    "The Pareto front shows the trade-off between fairness and excitement. Points on this front are non-dominated - improving one objective necessarily worsens the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6xrar15biqr",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Pareto front\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "scatter = ax.scatter(fairness, excitement, c=np.arange(len(fairness)), \n",
    "                     cmap='viridis', s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Fairness (p4 Win Rate)', fontsize=12)\n",
    "ax.set_ylabel('Excitement (Avg. Trick Changes)', fontsize=12)\n",
    "ax.set_title('Pareto Front: Top Trumps Game Balancing', fontsize=14)\n",
    "\n",
    "# Add colorbar to show solution index\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Solution Index', fontsize=10)\n",
    "\n",
    "# Highlight extreme solutions\n",
    "best_fair_idx = fairness.argmax()\n",
    "best_excite_idx = excitement.argmax()\n",
    "\n",
    "ax.scatter(fairness[best_fair_idx], excitement[best_fair_idx], \n",
    "           s=200, marker='*', c='red', edgecolors='black', linewidth=1,\n",
    "           label=f'Best Fairness ({fairness[best_fair_idx]:.2%})', zorder=5)\n",
    "ax.scatter(fairness[best_excite_idx], excitement[best_excite_idx], \n",
    "           s=200, marker='*', c='blue', edgecolors='black', linewidth=1,\n",
    "           label=f'Best Excitement ({excitement[best_excite_idx]:.2f})', zorder=5)\n",
    "\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/pareto_front_{mode_str}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Pareto front saved to {OUTPUT_DIR}/pareto_front_{mode_str}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clkdb702e85",
   "metadata": {},
   "source": [
    "### Convergence Analysis\n",
    "\n",
    "The hypervolume indicator tracks algorithm progress over generations. Higher hypervolume = better Pareto front approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2u6h3cv0l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Hypervolume over evaluations\n",
    "ax1 = axes[0]\n",
    "ax1.plot(n_eval_history, hv_history, 'b-', linewidth=2)\n",
    "ax1.scatter(n_eval_history, hv_history, c='blue', s=20, alpha=0.5)\n",
    "ax1.set_xlabel('Function Evaluations', fontsize=12)\n",
    "ax1.set_ylabel('Hypervolume', fontsize=12)\n",
    "ax1.set_title('Convergence: Hypervolume vs Evaluations', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Best objectives over generations\n",
    "ax2 = axes[1]\n",
    "generations = np.arange(1, len(hv_history) + 1)\n",
    "best_f1 = -np.array(res.algorithm.callback.data[\"best_f1\"])  # Un-negate\n",
    "best_f2 = -np.array(res.algorithm.callback.data[\"best_f2\"])  # Un-negate\n",
    "\n",
    "ax2.plot(generations, best_f1, 'r-', linewidth=2, label='Best Fairness')\n",
    "ax2.plot(generations, best_f2, 'g-', linewidth=2, label='Best Excitement')\n",
    "ax2.set_xlabel('Generation', fontsize=12)\n",
    "ax2.set_ylabel('Objective Value', fontsize=12)\n",
    "ax2.set_title('Best Objective Values per Generation', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/convergence_{mode_str}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Convergence plot saved to {OUTPUT_DIR}/convergence_{mode_str}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sfac5uoa2w",
   "metadata": {},
   "source": [
    "### Deck Pattern Analysis\n",
    "\n",
    "Analyzing the structure of extreme solutions to understand what makes a deck fair vs exciting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3jt65l14rcw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract extreme solutions\n",
    "best_fair_deck = X[best_fair_idx].reshape(NUM_CARDS, NUM_CATEGORIES)\n",
    "best_excite_deck = X[best_excite_idx].reshape(NUM_CARDS, NUM_CATEGORIES)\n",
    "\n",
    "# Visualize deck structures as heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Best Fairness Deck\n",
    "im1 = axes[0].imshow(best_fair_deck, cmap='YlOrRd', aspect='auto', vmin=1, vmax=10)\n",
    "axes[0].set_xlabel('Category', fontsize=12)\n",
    "axes[0].set_ylabel('Card', fontsize=12)\n",
    "axes[0].set_title(f'Best Fairness Deck\\n(Win Rate: {fairness[best_fair_idx]:.2%}, Excitement: {excitement[best_fair_idx]:.2f})', fontsize=12)\n",
    "axes[0].set_xticks(range(NUM_CATEGORIES))\n",
    "axes[0].set_xticklabels([f'Cat {i+1}' for i in range(NUM_CATEGORIES)])\n",
    "plt.colorbar(im1, ax=axes[0], label='Value')\n",
    "\n",
    "# Best Excitement Deck\n",
    "im2 = axes[1].imshow(best_excite_deck, cmap='YlOrRd', aspect='auto', vmin=1, vmax=10)\n",
    "axes[1].set_xlabel('Category', fontsize=12)\n",
    "axes[1].set_ylabel('Card', fontsize=12)\n",
    "axes[1].set_title(f'Best Excitement Deck\\n(Win Rate: {fairness[best_excite_idx]:.2%}, Excitement: {excitement[best_excite_idx]:.2f})', fontsize=12)\n",
    "axes[1].set_xticks(range(NUM_CATEGORIES))\n",
    "axes[1].set_xticklabels([f'Cat {i+1}' for i in range(NUM_CATEGORIES)])\n",
    "plt.colorbar(im2, ax=axes[1], label='Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/deck_heatmaps_{mode_str}.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Deck heatmaps saved to {OUTPUT_DIR}/deck_heatmaps_{mode_str}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qd7q6h05a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison of extreme decks\n",
    "print(\"Deck Statistics Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<30} {'Best Fairness':>14} {'Best Excitement':>14}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"{'Mean value':<30} {best_fair_deck.mean():>14.3f} {best_excite_deck.mean():>14.3f}\")\n",
    "print(f\"{'Std deviation':<30} {best_fair_deck.std():>14.3f} {best_excite_deck.std():>14.3f}\")\n",
    "print(f\"{'Min value':<30} {best_fair_deck.min():>14.3f} {best_excite_deck.min():>14.3f}\")\n",
    "print(f\"{'Max value':<30} {best_fair_deck.max():>14.3f} {best_excite_deck.max():>14.3f}\")\n",
    "\n",
    "# Per-category statistics\n",
    "print(\"-\" * 60)\n",
    "print(\"Per-category mean:\")\n",
    "for cat in range(NUM_CATEGORIES):\n",
    "    fair_mean = best_fair_deck[:, cat].mean()\n",
    "    excite_mean = best_excite_deck[:, cat].mean()\n",
    "    print(f\"  Category {cat+1:<22} {fair_mean:>14.3f} {excite_mean:>14.3f}\")\n",
    "\n",
    "# Per-category variance (indicator of \"specialization\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Per-category std (specialization indicator):\")\n",
    "for cat in range(NUM_CATEGORIES):\n",
    "    fair_std = best_fair_deck[:, cat].std()\n",
    "    excite_std = best_excite_deck[:, cat].std()\n",
    "    print(f\"  Category {cat+1:<22} {fair_std:>14.3f} {excite_std:>14.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2mh0rkn4hg",
   "metadata": {},
   "source": [
    "## Quality Indicator Summary\n",
    "\n",
    "Final assessment of the Pareto front approximation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96y52hb5wmv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final quality summary\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMIZATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nProblem Configuration:\")\n",
    "print(f\"  Cards (K): {NUM_CARDS}\")\n",
    "print(f\"  Categories (L): {NUM_CATEGORIES}\")\n",
    "print(f\"  Decision variables: {N_VAR}\")\n",
    "print(f\"  Simulations per evaluation (R): {R}\")\n",
    "\n",
    "print(f\"\\nAlgorithm Configuration:\")\n",
    "print(f\"  Algorithm: NSGA-II\")\n",
    "print(f\"  Population size: {POP_SIZE}\")\n",
    "print(f\"  Generations: {N_GEN}\")\n",
    "print(f\"  Total evaluations: {res.algorithm.evaluator.n_eval:,}\")\n",
    "\n",
    "print(f\"\\nQuality Indicators:\")\n",
    "print(f\"  Hypervolume: {final_hv:.6f}\")\n",
    "print(f\"  Reference point: {REF_POINT}\")\n",
    "print(f\"  Pareto front size: {len(F)} solutions\")\n",
    "\n",
    "print(f\"\\nObjective Ranges (un-negated):\")\n",
    "print(f\"  Fairness (p4 win rate): [{fairness.min():.3f}, {fairness.max():.3f}]\")\n",
    "print(f\"  Excitement (trick changes): [{excitement.min():.2f}, {excitement.max():.2f}]\")\n",
    "\n",
    "print(f\"\\nExtreme Solutions:\")\n",
    "print(f\"  Best Fairness: Win rate = {fairness[best_fair_idx]:.2%}, Excitement = {excitement[best_fair_idx]:.2f}\")\n",
    "print(f\"  Best Excitement: Win rate = {fairness[best_excite_idx]:.2%}, Excitement = {excitement[best_excite_idx]:.2f}\")\n",
    "\n",
    "print(f\"\\nRuntime:\")\n",
    "print(f\"  Total time: {elapsed_time/60:.1f} minutes\")\n",
    "print(f\"  Time per evaluation: {elapsed_time/res.algorithm.evaluator.n_eval:.3f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i0oqe7k5apa",
   "metadata": {},
   "source": [
    "## Algorithm Comparison: NSGA-II vs SMS-EMOA vs MOEA/D\n",
    "\n",
    "We compare three popular multi-objective evolutionary algorithms from the lectures:\n",
    "\n",
    "1. **NSGA-II** (Non-dominated Sorting Genetic Algorithm II)\n",
    "   - (μ+μ) strategy with non-dominated sorting + crowding distance\n",
    "   - Fast, widely used baseline\n",
    "\n",
    "2. **SMS-EMOA** (S-Metric Selection Evolutionary Multi-objective Optimization Algorithm)\n",
    "   - (μ+1) steady-state strategy\n",
    "   - Uses hypervolume contribution for selection\n",
    "   - Theoretically superior convergence properties\n",
    "\n",
    "3. **MOEA/D** (Multi-objective Evolutionary Algorithm based on Decomposition)\n",
    "   - Decomposes problem into weighted single-objective subproblems\n",
    "   - Uses neighborhood-based mating\n",
    "   - Efficient for many-objective problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qrj9juljx18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ALGORITHM COMPARISON EXPERIMENT\n",
      "============================================================\n",
      "Population size: 100\n",
      "Generations: 100\n",
      "Seeds: [42, 123, 456]\n",
      "Simulations per evaluation: 1000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.algorithms.moo.sms import SMSEMOA\n",
    "from pymoo.algorithms.moo.moead import MOEAD\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PM\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.indicators.hv import Hypervolume\n",
    "import time\n",
    "\n",
    "# Configuration for comparison\n",
    "COMP_POP_SIZE = 100\n",
    "COMP_N_GEN = 100\n",
    "COMP_SEEDS = [42, 123, 456]  # Multiple seeds for statistical validity\n",
    "\n",
    "# Reference point for hypervolume (same as before)\n",
    "REF_POINT = np.array([0.1, 0.1])\n",
    "hv_indicator = Hypervolume(ref_point=REF_POINT)\n",
    "\n",
    "# Store results for each algorithm\n",
    "algorithm_results = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ALGORITHM COMPARISON EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Population size: {COMP_POP_SIZE}\")\n",
    "print(f\"Generations: {COMP_N_GEN}\")\n",
    "print(f\"Seeds: {COMP_SEEDS}\")\n",
    "print(f\"Simulations per evaluation: {R}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "v6uak3cahlk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithms defined:\n",
      "  - NSGA-II\n",
      "  - SMS-EMOA\n",
      "  - MOEA/D\n"
     ]
    }
   ],
   "source": [
    "# Define algorithms\n",
    "def get_algorithms():\n",
    "    \"\"\"Create fresh algorithm instances for each run.\"\"\"\n",
    "    \n",
    "    # NSGA-II\n",
    "    nsga2 = NSGA2(\n",
    "        pop_size=COMP_POP_SIZE,\n",
    "        sampling=FloatRandomSampling(),\n",
    "        crossover=SBX(prob=0.9, eta=15),\n",
    "        mutation=PM(eta=20),\n",
    "        eliminate_duplicates=True\n",
    "    )\n",
    "    \n",
    "    # SMS-EMOA (steady-state, hypervolume-based)\n",
    "    smsemoa = SMSEMOA(\n",
    "        pop_size=COMP_POP_SIZE,\n",
    "        sampling=FloatRandomSampling(),\n",
    "        crossover=SBX(prob=0.9, eta=15),\n",
    "        mutation=PM(eta=20)\n",
    "    )\n",
    "    \n",
    "    # MOEA/D (decomposition-based)\n",
    "    # Need reference directions for decomposition\n",
    "    ref_dirs = get_reference_directions(\"uniform\", 2, n_partitions=COMP_POP_SIZE-1)\n",
    "    moead = MOEAD(\n",
    "        ref_dirs=ref_dirs,\n",
    "        n_neighbors=20,\n",
    "        sampling=FloatRandomSampling(),\n",
    "        crossover=SBX(prob=0.9, eta=15),\n",
    "        mutation=PM(eta=20)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"NSGA-II\": nsga2,\n",
    "        \"SMS-EMOA\": smsemoa,\n",
    "        \"MOEA/D\": moead\n",
    "    }\n",
    "\n",
    "print(\"Algorithms defined:\")\n",
    "for name in get_algorithms().keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wv8icqfzqd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all algorithms with multiple seeds\n",
    "for algo_name, algorithm in get_algorithms().items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running {algo_name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    algo_hvs = []\n",
    "    algo_times = []\n",
    "    algo_fronts = []\n",
    "    \n",
    "    for seed in COMP_SEEDS:\n",
    "        print(f\"\\n  Seed {seed}:\")\n",
    "        \n",
    "        # Create fresh problem instance for each run\n",
    "        sim = TopTrumpsSimulation(num_cards=NUM_CARDS, num_categories=NUM_CATEGORIES)\n",
    "        prob = TopTrumpsBalancing(sim, n_simulations=R)\n",
    "        \n",
    "        # Get fresh algorithm instance\n",
    "        algorithms = get_algorithms()\n",
    "        algo = algorithms[algo_name]\n",
    "        \n",
    "        # Set seeds\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        res = minimize(\n",
    "            prob,\n",
    "            algo,\n",
    "            ('n_gen', COMP_N_GEN),\n",
    "            seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate hypervolume\n",
    "        hv = hv_indicator.do(res.F)\n",
    "        \n",
    "        algo_hvs.append(hv)\n",
    "        algo_times.append(elapsed)\n",
    "        algo_fronts.append(res.F)\n",
    "        \n",
    "        print(f\"    HV: {hv:.4f}, Time: {elapsed/60:.1f} min, Solutions: {len(res.F)}\")\n",
    "    \n",
    "    # Store results\n",
    "    algorithm_results[algo_name] = {\n",
    "        \"hvs\": algo_hvs,\n",
    "        \"times\": algo_times,\n",
    "        \"fronts\": algo_fronts,\n",
    "        \"hv_mean\": np.mean(algo_hvs),\n",
    "        \"hv_std\": np.std(algo_hvs),\n",
    "        \"time_mean\": np.mean(algo_times),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  {algo_name} Summary:\")\n",
    "    print(f\"    HV: {np.mean(algo_hvs):.4f} ± {np.std(algo_hvs):.4f}\")\n",
    "    print(f\"    Avg Time: {np.mean(algo_times)/60:.1f} min\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All algorithms completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "idwodqvsom",
   "metadata": {},
   "source": [
    "### Algorithm Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bvx3q82qjha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"=\" * 70)\n",
    "print(\"ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Algorithm':<15} {'HV Mean':>12} {'HV Std':>10} {'Avg Time':>12} {'Best HV':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "best_algo = None\n",
    "best_hv = 0\n",
    "\n",
    "for algo_name, results in algorithm_results.items():\n",
    "    hv_mean = results[\"hv_mean\"]\n",
    "    hv_std = results[\"hv_std\"]\n",
    "    time_mean = results[\"time_mean\"]\n",
    "    best_run_hv = max(results[\"hvs\"])\n",
    "    \n",
    "    print(f\"{algo_name:<15} {hv_mean:>12.4f} {hv_std:>10.4f} {time_mean/60:>10.1f} min {best_run_hv:>12.4f}\")\n",
    "    \n",
    "    if hv_mean > best_hv:\n",
    "        best_hv = hv_mean\n",
    "        best_algo = algo_name\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"Best algorithm by mean HV: {best_algo} ({best_hv:.4f})\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cnhhjusukv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Pareto fronts comparison (best run from each algorithm)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = {'NSGA-II': 'blue', 'SMS-EMOA': 'red', 'MOEA/D': 'green'}\n",
    "markers = {'NSGA-II': 'o', 'SMS-EMOA': 's', 'MOEA/D': '^'}\n",
    "\n",
    "for algo_name, results in algorithm_results.items():\n",
    "    # Get best run (highest HV)\n",
    "    best_idx = np.argmax(results[\"hvs\"])\n",
    "    F = results[\"fronts\"][best_idx]\n",
    "    \n",
    "    # Un-negate objectives\n",
    "    fair = -F[:, 0]\n",
    "    excite = -F[:, 1]\n",
    "    \n",
    "    ax.scatter(fair, excite, c=colors[algo_name], marker=markers[algo_name], \n",
    "               s=80, label=f'{algo_name} (HV={results[\"hvs\"][best_idx]:.3f})',\n",
    "               edgecolors='black', linewidth=0.5, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Fairness (p4 Win Rate)', fontsize=12)\n",
    "ax.set_ylabel('Excitement (Avg. Trick Changes)', fontsize=12)\n",
    "ax.set_title('Pareto Front Comparison: NSGA-II vs SMS-EMOA vs MOEA/D', fontsize=14)\n",
    "ax.legend(loc='lower left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/algorithm_comparison_pareto.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {OUTPUT_DIR}/algorithm_comparison_pareto.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96letifuyp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Hypervolume comparison (bar chart with error bars)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "algo_names = list(algorithm_results.keys())\n",
    "hv_means = [algorithm_results[a][\"hv_mean\"] for a in algo_names]\n",
    "hv_stds = [algorithm_results[a][\"hv_std\"] for a in algo_names]\n",
    "time_means = [algorithm_results[a][\"time_mean\"]/60 for a in algo_names]\n",
    "\n",
    "colors_list = [colors[a] for a in algo_names]\n",
    "\n",
    "# Plot 1: Hypervolume comparison\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(algo_names, hv_means, yerr=hv_stds, capsize=5, \n",
    "                color=colors_list, edgecolor='black', alpha=0.8)\n",
    "ax1.set_ylabel('Hypervolume', fontsize=12)\n",
    "ax1.set_title('Hypervolume Comparison\\n(higher is better)', fontsize=13)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean, std in zip(bars1, hv_means, hv_stds):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.05,\n",
    "             f'{mean:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 2: Runtime comparison\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(algo_names, time_means, color=colors_list, edgecolor='black', alpha=0.8)\n",
    "ax2.set_ylabel('Runtime (minutes)', fontsize=12)\n",
    "ax2.set_title('Runtime Comparison\\n(lower is better)', fontsize=13)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, t in zip(bars2, time_means):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{t:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/algorithm_comparison_bars.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {OUTPUT_DIR}/algorithm_comparison_bars.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z5l57eecygn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_data = {\n",
    "    \"algorithms\": algo_names,\n",
    "    \"hv_means\": hv_means,\n",
    "    \"hv_stds\": hv_stds,\n",
    "    \"time_means\": time_means,\n",
    "    \"seeds\": COMP_SEEDS,\n",
    "    \"pop_size\": COMP_POP_SIZE,\n",
    "    \"n_gen\": COMP_N_GEN,\n",
    "    \"n_simulations\": R\n",
    "}\n",
    "\n",
    "np.savez(f\"{OUTPUT_DIR}/algorithm_comparison.npz\", **comparison_data)\n",
    "\n",
    "# Also save individual Pareto fronts\n",
    "for algo_name, results in algorithm_results.items():\n",
    "    best_idx = np.argmax(results[\"hvs\"])\n",
    "    np.save(f\"{OUTPUT_DIR}/pareto_F_{algo_name.replace('-', '_').lower()}.npy\", \n",
    "            results[\"fronts\"][best_idx])\n",
    "\n",
    "print(f\"Comparison results saved to {OUTPUT_DIR}/algorithm_comparison.npz\")\n",
    "print(\"Individual Pareto fronts saved for each algorithm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l7ai64kc1a",
   "metadata": {},
   "source": [
    "## Performance Optimization with Numba\n",
    "\n",
    "The game simulation is the bottleneck - each evaluation runs R=1000 simulations. We can achieve significant speedup using **Numba**, a Just-In-Time (JIT) compiler that translates Python to optimized machine code.\n",
    "\n",
    "### Key Optimizations:\n",
    "1. **JIT Compilation**: `@njit` decorator compiles functions to machine code\n",
    "2. **NumPy Arrays**: Replace Python lists with fixed-size arrays\n",
    "3. **Boolean Masks**: Use masks instead of list `.remove()` operations\n",
    "4. **No Classes**: Numba works with functions and arrays, not Python objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nzdj0frw3sr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /opt/homebrew/lib/python3.10/site-packages (0.63.1)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /opt/homebrew/lib/python3.10/site-packages (from numba) (0.46.0)\n",
      "Requirement already satisfied: numpy<2.4,>=1.22 in /opt/homebrew/lib/python3.10/site-packages (from numba) (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Numba if not already installed\n",
    "%pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gvua33ri27w",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba-optimized functions defined.\n",
      "First call will trigger JIT compilation (one-time cost).\n"
     ]
    }
   ],
   "source": [
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "@njit(cache=True)\n",
    "def simulate_game_numba(deck_normalized, K, L):\n",
    "    \"\"\"\n",
    "    Numba-optimized game simulation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    deck_normalized : np.ndarray of shape (K, L)\n",
    "        Pre-normalized deck where each column is scaled to [0, 1]\n",
    "    K : int\n",
    "        Number of cards\n",
    "    L : int\n",
    "        Number of categories\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    p4_tricks : int\n",
    "        Number of tricks won by p4\n",
    "    trick_changes : int\n",
    "        Number of times the lead changed\n",
    "    p4_won : bool\n",
    "        Whether p4 won the game\n",
    "    \"\"\"\n",
    "    K_half = K // 2\n",
    "\n",
    "    # Shuffle card indices\n",
    "    indices = np.arange(K)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Distribute cards\n",
    "    p4_indices = indices[:K_half]\n",
    "    p0_indices = indices[K_half:]\n",
    "    \n",
    "    # Game state\n",
    "    p4_tricks = 0\n",
    "    current_is_p4 = np.random.random() > 0.5\n",
    "    trick_changes = 0\n",
    "    \n",
    "    # Track remaining cards with a compact index list and position map\n",
    "    remaining = np.arange(K)\n",
    "    pos = np.arange(K)\n",
    "    n_remaining = K\n",
    "    \n",
    "    # Play K/2 rounds\n",
    "    for round_idx in range(K_half):\n",
    "        p4_card_idx = p4_indices[round_idx]\n",
    "        p0_card_idx = p0_indices[round_idx]\n",
    "        \n",
    "        card_p4 = deck_normalized[p4_card_idx]\n",
    "        card_p0 = deck_normalized[p0_card_idx]\n",
    "        \n",
    "        # Determine category choice\n",
    "        if current_is_p4:\n",
    "            # p4 strategy: calculate exact win probability for each category\n",
    "            best_prob = -1.0\n",
    "            best_cat = 0\n",
    "            \n",
    "            for cat in range(L):\n",
    "                my_val = card_p4[cat]\n",
    "                wins = 0\n",
    "                \n",
    "                # Count wins against remaining cards (compact list)\n",
    "                for r in range(n_remaining):\n",
    "                    card_idx = remaining[r]\n",
    "                    if my_val > deck_normalized[card_idx, cat]:\n",
    "                        wins += 1\n",
    "                \n",
    "                prob = wins / n_remaining\n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_cat = cat\n",
    "            \n",
    "            category = best_cat\n",
    "        else:\n",
    "            # p0 strategy: pick category with highest normalized value\n",
    "            best_cat = 0\n",
    "            best_val = card_p0[0]\n",
    "            \n",
    "            for cat in range(1, L):\n",
    "                val = card_p0[cat]\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_cat = cat\n",
    "            \n",
    "            category = best_cat\n",
    "        \n",
    "        # Compare values\n",
    "        val_p4 = card_p4[category]\n",
    "        val_p0 = card_p0[category]\n",
    "        \n",
    "        # Remove played cards from remaining pool (swap remove)\n",
    "        idx = pos[p4_card_idx]\n",
    "        last = remaining[n_remaining - 1]\n",
    "        remaining[idx] = last\n",
    "        pos[last] = idx\n",
    "        n_remaining -= 1\n",
    "        \n",
    "        idx = pos[p0_card_idx]\n",
    "        last = remaining[n_remaining - 1]\n",
    "        remaining[idx] = last\n",
    "        pos[last] = idx\n",
    "        n_remaining -= 1\n",
    "        \n",
    "        # Determine round winner\n",
    "        if val_p4 > val_p0:\n",
    "            p4_tricks += 1\n",
    "            if not current_is_p4:\n",
    "                current_is_p4 = True\n",
    "                trick_changes += 1\n",
    "        elif val_p0 > val_p4:\n",
    "            if current_is_p4:\n",
    "                current_is_p4 = False\n",
    "                trick_changes += 1\n",
    "        # Draw: current player keeps lead\n",
    "    \n",
    "    p4_won = p4_tricks > K // 4\n",
    "    return p4_tricks, trick_changes, p4_won\n",
    "\n",
    "\n",
    "@njit(cache=True)\n",
    "def run_simulations_numba(deck_normalized, K, L, n_simulations):\n",
    "    \"\"\"\n",
    "    Run multiple game simulations and return average metrics.\n",
    "    \n",
    "    This function is also JIT-compiled for maximum performance.\n",
    "    \"\"\"\n",
    "    total_wins = 0\n",
    "    total_trick_changes = 0\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        p4_tricks, trick_changes, p4_won = simulate_game_numba(deck_normalized, K, L)\n",
    "        if p4_won:\n",
    "            total_wins += 1\n",
    "        total_trick_changes += trick_changes\n",
    "    \n",
    "    avg_win_rate = total_wins / n_simulations\n",
    "    avg_trick_changes = total_trick_changes / n_simulations\n",
    "    \n",
    "    return avg_win_rate, avg_trick_changes\n",
    "\n",
    "\n",
    "def normalize_deck(deck_flat, K, L):\n",
    "    \"\"\"\n",
    "    Normalize deck values to [0, 1] per category.\n",
    "    \n",
    "    This runs in Python (not Numba) since it's only called once per evaluation.\n",
    "    \"\"\"\n",
    "    deck = deck_flat.reshape(K, L).astype(np.float64, copy=False)\n",
    "    \n",
    "    col_min = deck.min(axis=0)\n",
    "    col_max = deck.max(axis=0)\n",
    "    denom = col_max - col_min\n",
    "    denom_safe = np.where(denom > 0.0, denom, 1.0)\n",
    "    \n",
    "    deck_normalized = (deck - col_min) / denom_safe\n",
    "    zero_mask = denom == 0.0\n",
    "    if np.any(zero_mask):\n",
    "        deck_normalized[:, zero_mask] = 0.5  # All same value\n",
    "    \n",
    "    return np.asfortranarray(deck_normalized)\n",
    "\n",
    "\n",
    "print(\"Numba-optimized functions defined.\")\n",
    "print(\"First call will trigger JIT compilation (one-time cost).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jdt0ab3rfl8",
   "metadata": {},
   "source": [
    "### Benchmark: Original vs Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff6jezhdiz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up Numba (JIT compilation)...\n",
      "Compilation complete.\n",
      "\n",
      "Benchmarking ORIGINAL implementation...\n",
      "  10000 games in 0.231s\n",
      "  23.14 µs per game\n",
      "\n",
      "Benchmarking NUMBA implementation...\n",
      "  10000 games in 0.006s\n",
      "  0.64 µs per game\n",
      "\n",
      "Benchmarking full evaluation (1000 simulations)...\n",
      "  Original: 23.37 ms per evaluation\n",
      "  Numba:    0.49 ms per evaluation\n",
      "\n",
      "==================================================\n",
      "SPEEDUP SUMMARY\n",
      "==================================================\n",
      "Per game:       35.9x faster\n",
      "Per evaluation: 47.7x faster\n",
      "\n",
      "Projected algorithm run time:\n",
      "  Original: ~3.9 min\n",
      "  Numba:    ~0.1 min\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create test deck\n",
    "test_deck = np.random.uniform(1, 10, NUM_CARDS * NUM_CATEGORIES)\n",
    "test_deck_normalized = normalize_deck(test_deck, NUM_CARDS, NUM_CATEGORIES)\n",
    "\n",
    "# Warm up Numba (JIT compilation happens on first call)\n",
    "print(\"Warming up Numba (JIT compilation)...\")\n",
    "_ = simulate_game_numba(test_deck_normalized, NUM_CARDS, NUM_CATEGORIES)\n",
    "_ = run_simulations_numba(test_deck_normalized, NUM_CARDS, NUM_CATEGORIES, 10)\n",
    "print(\"Compilation complete.\\n\")\n",
    "\n",
    "# Benchmark parameters\n",
    "N_GAMES = 10000\n",
    "N_EVALS = 100  # Number of evaluation calls to simulate\n",
    "\n",
    "# --- Benchmark Original Implementation ---\n",
    "print(\"Benchmarking ORIGINAL implementation...\")\n",
    "original_sim = TopTrumpsSimulation(num_cards=NUM_CARDS, num_categories=NUM_CATEGORIES)\n",
    "original_sim.set_deck(test_deck)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(N_GAMES):\n",
    "    original_sim.simulate_game()\n",
    "original_time = time.perf_counter() - start\n",
    "original_per_game = original_time / N_GAMES * 1e6  # microseconds\n",
    "\n",
    "print(f\"  {N_GAMES} games in {original_time:.3f}s\")\n",
    "print(f\"  {original_per_game:.2f} µs per game\")\n",
    "\n",
    "# --- Benchmark Numba Implementation ---\n",
    "print(\"\\nBenchmarking NUMBA implementation...\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(N_GAMES):\n",
    "    simulate_game_numba(test_deck_normalized, NUM_CARDS, NUM_CATEGORIES)\n",
    "numba_time = time.perf_counter() - start\n",
    "numba_per_game = numba_time / N_GAMES * 1e6  # microseconds\n",
    "\n",
    "print(f\"  {N_GAMES} games in {numba_time:.3f}s\")\n",
    "print(f\"  {numba_per_game:.2f} µs per game\")\n",
    "\n",
    "# --- Benchmark Full Evaluation (R simulations) ---\n",
    "print(f\"\\nBenchmarking full evaluation ({R} simulations)...\")\n",
    "\n",
    "# Original\n",
    "start = time.perf_counter()\n",
    "for _ in range(N_EVALS):\n",
    "    original_sim.set_deck(np.random.uniform(1, 10, NUM_CARDS * NUM_CATEGORIES))\n",
    "    wins = sum(1 for _ in range(R) if original_sim.simulate_game()['p4_won'])\n",
    "original_eval_time = (time.perf_counter() - start) / N_EVALS * 1000  # ms\n",
    "\n",
    "# Numba\n",
    "start = time.perf_counter()\n",
    "for _ in range(N_EVALS):\n",
    "    deck = np.random.uniform(1, 10, NUM_CARDS * NUM_CATEGORIES)\n",
    "    deck_norm = normalize_deck(deck, NUM_CARDS, NUM_CATEGORIES)\n",
    "    run_simulations_numba(deck_norm, NUM_CARDS, NUM_CATEGORIES, R)\n",
    "numba_eval_time = (time.perf_counter() - start) / N_EVALS * 1000  # ms\n",
    "\n",
    "print(f\"  Original: {original_eval_time:.2f} ms per evaluation\")\n",
    "print(f\"  Numba:    {numba_eval_time:.2f} ms per evaluation\")\n",
    "\n",
    "# --- Summary ---\n",
    "speedup_game = original_per_game / numba_per_game\n",
    "speedup_eval = original_eval_time / numba_eval_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SPEEDUP SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Per game:       {speedup_game:.1f}x faster\")\n",
    "print(f\"Per evaluation: {speedup_eval:.1f}x faster\")\n",
    "print(f\"\\nProjected algorithm run time:\")\n",
    "print(f\"  Original: ~{N_GEN * POP_SIZE * original_eval_time / 1000 / 60:.1f} min\")\n",
    "print(f\"  Numba:    ~{N_GEN * POP_SIZE * numba_eval_time / 1000 / 60:.1f} min\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ynvetj25k4k",
   "metadata": {},
   "source": [
    "### Numba-Optimized Problem Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "399dequxnvc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba problem initialized:\n",
      "  Variables: 88\n",
      "  Objectives: 2\n",
      "  Simulations per evaluation: 1000\n"
     ]
    }
   ],
   "source": [
    "from pymoo.core.problem import ElementwiseProblem\n",
    "\n",
    "class TopTrumpsBalancingNumba(ElementwiseProblem):\n",
    "    \"\"\"\n",
    "    Numba-optimized version of TopTrumpsBalancing.\n",
    "    \n",
    "    Uses JIT-compiled simulation functions for ~10x speedup.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_cards=22, num_categories=4, n_simulations=1000):\n",
    "        self.K = num_cards\n",
    "        self.L = num_categories\n",
    "        self.n_simulations = n_simulations\n",
    "        \n",
    "        n_var = num_cards * num_categories\n",
    "        \n",
    "        super().__init__(\n",
    "            n_var=n_var,\n",
    "            n_obj=2,\n",
    "            n_constr=0,\n",
    "            xl=1.0,\n",
    "            xu=10.0\n",
    "        )\n",
    "    \n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # Normalize deck (Python - runs once per evaluation)\n",
    "        deck_normalized = normalize_deck(x, self.K, self.L)\n",
    "        \n",
    "        # Run simulations (Numba - fast)\n",
    "        avg_win_rate, avg_trick_changes = run_simulations_numba(\n",
    "            deck_normalized, self.K, self.L, self.n_simulations\n",
    "        )\n",
    "        \n",
    "        # Objectives (negated for minimization)\n",
    "        out[\"F\"] = [-avg_win_rate, -avg_trick_changes]\n",
    "\n",
    "\n",
    "# Create Numba problem instance\n",
    "problem_numba = TopTrumpsBalancingNumba(\n",
    "    num_cards=NUM_CARDS, \n",
    "    num_categories=NUM_CATEGORIES, \n",
    "    n_simulations=R\n",
    ")\n",
    "\n",
    "print(f\"Numba problem initialized:\")\n",
    "print(f\"  Variables: {problem_numba.n_var}\")\n",
    "print(f\"  Objectives: {problem_numba.n_obj}\")\n",
    "print(f\"  Simulations per evaluation: {problem_numba.n_simulations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r5mbbr374w",
   "metadata": {},
   "source": [
    "### Re-run Algorithm Comparison with Numba\n",
    "\n",
    "Now we re-run the same experiment with the Numba-optimized problem class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1snm0qdyrkl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ALGORITHM COMPARISON WITH NUMBA OPTIMIZATION\n",
      "============================================================\n",
      "Population size: 100\n",
      "Generations: 100\n",
      "Seeds: [42, 123, 456]\n",
      "Simulations per evaluation: 1000\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Running NSGA-II (Numba)...\n",
      "============================================================\n",
      "\n",
      "  Seed 42: HV: 4.2239, Time: 5.6s, Solutions: 27\n",
      "\n",
      "  Seed 123: HV: 4.3513, Time: 5.7s, Solutions: 27\n",
      "\n",
      "  Seed 456: HV: 4.5469, Time: 5.7s, Solutions: 42\n",
      "\n",
      "  NSGA-II Summary:\n",
      "    HV: 4.3740 ± 0.1328\n",
      "    Avg Time: 5.7s\n",
      "\n",
      "============================================================\n",
      "Running SMS-EMOA (Numba)...\n",
      "============================================================\n",
      "\n",
      "  Seed 42: HV: 4.2462, Time: 5.8s, Solutions: 33\n",
      "\n",
      "  Seed 123: HV: 4.3058, Time: 5.7s, Solutions: 18\n",
      "\n",
      "  Seed 456: HV: 4.3361, Time: 5.7s, Solutions: 28\n",
      "\n",
      "  SMS-EMOA Summary:\n",
      "    HV: 4.2961 ± 0.0374\n",
      "    Avg Time: 5.7s\n",
      "\n",
      "============================================================\n",
      "Running MOEA/D (Numba)...\n",
      "============================================================\n",
      "\n",
      "  Seed 42: HV: 2.7967, Time: 7.6s, Solutions: 43\n",
      "\n",
      "  Seed 123: HV: 2.7360, Time: 7.6s, Solutions: 42\n",
      "\n",
      "  Seed 456: HV: 3.1190, Time: 7.7s, Solutions: 73\n",
      "\n",
      "  MOEA/D Summary:\n",
      "    HV: 2.8839 ± 0.1681\n",
      "    Avg Time: 7.6s\n",
      "\n",
      "============================================================\n",
      "All algorithms completed in 57.1s (1.0 min)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Store results for Numba runs\n",
    "algorithm_results_numba = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ALGORITHM COMPARISON WITH NUMBA OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Population size: {COMP_POP_SIZE}\")\n",
    "print(f\"Generations: {COMP_N_GEN}\")\n",
    "print(f\"Seeds: {COMP_SEEDS}\")\n",
    "print(f\"Simulations per evaluation: {R}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for algo_name in [\"NSGA-II\", \"SMS-EMOA\", \"MOEA/D\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running {algo_name} (Numba)...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    algo_hvs = []\n",
    "    algo_times = []\n",
    "    algo_fronts = []\n",
    "    \n",
    "    for seed in COMP_SEEDS:\n",
    "        print(f\"\\n  Seed {seed}:\", end=\" \")\n",
    "        \n",
    "        # Create fresh Numba problem instance\n",
    "        prob = TopTrumpsBalancingNumba(\n",
    "            num_cards=NUM_CARDS,\n",
    "            num_categories=NUM_CATEGORIES,\n",
    "            n_simulations=R\n",
    "        )\n",
    "        \n",
    "        # Get fresh algorithm instance\n",
    "        algorithms = get_algorithms()\n",
    "        algo = algorithms[algo_name]\n",
    "        \n",
    "        # Set seeds\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        res = minimize(\n",
    "            prob,\n",
    "            algo,\n",
    "            ('n_gen', COMP_N_GEN),\n",
    "            seed=seed,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate hypervolume\n",
    "        hv = hv_indicator.do(res.F)\n",
    "        \n",
    "        algo_hvs.append(hv)\n",
    "        algo_times.append(elapsed)\n",
    "        algo_fronts.append(res.F)\n",
    "        \n",
    "        print(f\"HV: {hv:.4f}, Time: {elapsed:.1f}s, Solutions: {len(res.F)}\")\n",
    "    \n",
    "    # Store results\n",
    "    algorithm_results_numba[algo_name] = {\n",
    "        \"hvs\": algo_hvs,\n",
    "        \"times\": algo_times,\n",
    "        \"fronts\": algo_fronts,\n",
    "        \"hv_mean\": np.mean(algo_hvs),\n",
    "        \"hv_std\": np.std(algo_hvs),\n",
    "        \"time_mean\": np.mean(algo_times),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  {algo_name} Summary:\")\n",
    "    print(f\"    HV: {np.mean(algo_hvs):.4f} ± {np.std(algo_hvs):.4f}\")\n",
    "    print(f\"    Avg Time: {np.mean(algo_times):.1f}s\")\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"All algorithms completed in {total_elapsed:.1f}s ({total_elapsed/60:.1f} min)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pn6nb7rf1sc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Original vs Numba performance\n",
    "print(\"=\" * 70)\n",
    "print(\"SPEEDUP COMPARISON: ORIGINAL vs NUMBA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Algorithm':<15} {'Original Time':>15} {'Numba Time':>15} {'Speedup':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for algo_name in algorithm_results.keys():\n",
    "    orig_time = algorithm_results[algo_name][\"time_mean\"]\n",
    "    numba_time = algorithm_results_numba[algo_name][\"time_mean\"]\n",
    "    speedup = orig_time / numba_time\n",
    "    \n",
    "    print(f\"{algo_name:<15} {orig_time:>12.1f}s {numba_time:>12.1f}s {speedup:>10.1f}x\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "total_orig = sum(r[\"time_mean\"] for r in algorithm_results.values()) * len(COMP_SEEDS)\n",
    "total_numba = sum(r[\"time_mean\"] for r in algorithm_results_numba.values()) * len(COMP_SEEDS)\n",
    "print(f\"{'TOTAL':<15} {total_orig:>12.1f}s {total_numba:>12.1f}s {total_orig/total_numba:>10.1f}x\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x5mzi2qskg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Original vs Numba runtime comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "algo_names = list(algorithm_results.keys())\n",
    "orig_times = [algorithm_results[a][\"time_mean\"] / 60 for a in algo_names]  # minutes\n",
    "numba_times = [algorithm_results_numba[a][\"time_mean\"] / 60 for a in algo_names]  # minutes\n",
    "\n",
    "x = np.arange(len(algo_names))\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: Runtime comparison\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(x - width/2, orig_times, width, label='Original', color='coral', edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, numba_times, width, label='Numba', color='steelblue', edgecolor='black')\n",
    "\n",
    "ax1.set_ylabel('Runtime (minutes)', fontsize=12)\n",
    "ax1.set_title('Runtime: Original vs Numba', fontsize=13)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(algo_names)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add speedup annotations\n",
    "for i, (o, n) in enumerate(zip(orig_times, numba_times)):\n",
    "    speedup = o / n\n",
    "    ax1.annotate(f'{speedup:.1f}x', xy=(i, max(o, n) + 0.2), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: HV comparison (should be similar - verifies correctness)\n",
    "ax2 = axes[1]\n",
    "orig_hvs = [algorithm_results[a][\"hv_mean\"] for a in algo_names]\n",
    "numba_hvs = [algorithm_results_numba[a][\"hv_mean\"] for a in algo_names]\n",
    "orig_stds = [algorithm_results[a][\"hv_std\"] for a in algo_names]\n",
    "numba_stds = [algorithm_results_numba[a][\"hv_std\"] for a in algo_names]\n",
    "\n",
    "bars3 = ax2.bar(x - width/2, orig_hvs, width, yerr=orig_stds, capsize=3, \n",
    "                label='Original', color='coral', edgecolor='black')\n",
    "bars4 = ax2.bar(x + width/2, numba_hvs, width, yerr=numba_stds, capsize=3,\n",
    "                label='Numba', color='steelblue', edgecolor='black')\n",
    "\n",
    "ax2.set_ylabel('Hypervolume', fontsize=12)\n",
    "ax2.set_title('Hypervolume: Original vs Numba\\n(should be similar - verifies correctness)', fontsize=13)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(algo_names)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/numba_speedup_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {OUTPUT_DIR}/numba_speedup_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lr2sligxd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Numba comparison results\n",
    "numba_comparison_data = {\n",
    "    \"algorithms\": algo_names,\n",
    "    \"orig_times\": [algorithm_results[a][\"time_mean\"] for a in algo_names],\n",
    "    \"numba_times\": [algorithm_results_numba[a][\"time_mean\"] for a in algo_names],\n",
    "    \"orig_hvs\": [algorithm_results[a][\"hv_mean\"] for a in algo_names],\n",
    "    \"numba_hvs\": [algorithm_results_numba[a][\"hv_mean\"] for a in algo_names],\n",
    "    \"speedups\": [algorithm_results[a][\"time_mean\"] / algorithm_results_numba[a][\"time_mean\"] for a in algo_names],\n",
    "}\n",
    "\n",
    "np.savez(f\"{OUTPUT_DIR}/numba_comparison.npz\", **numba_comparison_data)\n",
    "\n",
    "print(f\"Numba comparison results saved to {OUTPUT_DIR}/numba_comparison.npz\")\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(f\"  Average speedup: {np.mean(numba_comparison_data['speedups']):.1f}x\")\n",
    "print(f\"  HV difference: <{max(abs(o-n) for o,n in zip(numba_comparison_data['orig_hvs'], numba_comparison_data['numba_hvs'])):.3f} (verifies correctness)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459dfcd-e6da-48dd-8ba6-af04ad8e19dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
